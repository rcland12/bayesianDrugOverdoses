CVerrors[i] <- sum((y-yhat)^2)
}
CVerrors
plot(h,CVerrors)               # visualize error as a function of h
minIndex = which.min(CVerrors) # Identify where the minimum error is
h[minIndex]                    # Extract corresponding value of h
n <- length(x)
is.invertible <- function(m){
class(try(solve(m),silent=T))=="matrix"
}
for (i in 1:length(h)) {
yhat <- rep(NA,length(x))
for (j in 1:length(x)) {
X <- cbind(rep(1,length(x[-j])),x[-j])
Y <- y[-j]
W = diag(K(x[j]-X[,2],h[i]))
if(is.invertible(t(X)%*%W%*%X)){
betaHat <- solve(t(X)%*%W%*%X,t(X)%*%W%*%Y)
yhat[j] <- betaHat[1]+betaHat[2]*x[j]
}
else{
yhat[j] = sum(K(x[j]-X[,2],h[i])*y[j])/sum(K(x[j]-X[,2],h[i]))
}
}
CVerrors[i] <- sum((y-yhat)^2)
}
plot(h,CVerrors)               # visualize error as a function of h
minIndex = which.min(CVerrors) # Identify where the minimum error is
h[minIndex]                    # Extract corresponding value of h
X = matrix(c(rep(1,n),x),nrow=n)
X
betaHatOLS = solve(t(X)%*%X,t(X)%*%y) # Testing matrix commands using ordinary least squares
betaHatOLS
plot(x,y)
abline(betaHatOLS[1],betaHatOLS[2])  # Obviously a terrible fit.
x = c(16.9,17.1,12.7,14.4,13.9,16.3,16.0,13.1,12.7,13.4,17.5,
14.9,16.0,17.8,15.7,17.2,15.4,12.4)
y = c(12.6,23.7,31.4,11.8,18.0,3.3,  4.6,30.4,30.4,28.4,23.1,
6.6,  .3,35.5,-1.1,18.7,  .2,30.1)
n=length(x)
X = matrix(c(rep(1,n),x),nrow=n)
X
betaHatOLS = solve(t(X)%*%X,t(X)%*%y) # Testing matrix commands using ordinary least squares
betaHatOLS
plot(x,y)
abline(betaHatOLS[1],betaHatOLS[2])  # Obviously a terrible fit.
h=1.2
x0=15                   # Do a local linear regression at this point
X = matrix(c(rep(1,n),x,x^2),nrow=n)
X
betaHatOLS = solve(t(X)%*%X,t(X)%*%y) # Testing matrix commands using ordinary least squares
betaHatOLS
plot(x,y)
abline(betaHatOLS[1],betaHatOLS[2])  # Obviously a terrible fit.
points <- seq(min(x),max(x),0.1)
points2 <- betaHatOLS[1]+betaHatOLS[2]*points+betaHatOLS*points
points2 <- betaHatOLS[1]+betaHatOLS[2]*points+betaHatOLS[3]*points
lines(points,points2)
X = matrix(c(rep(1,n),x,x^2),nrow=n)
X
betaHatOLS = solve(t(X)%*%X,t(X)%*%y) # Testing matrix commands using ordinary least squares
betaHatOLS
plot(x,y)
points <- seq(min(x),max(x),0.1)
points2 <- betaHatOLS[1]+betaHatOLS[2]*points+betaHatOLS[3]*points
lines(points,points2)
h=1.2
X = matrix(c(rep(1,n),x,x^2),nrow=n)
X
betaHatOLS = solve(t(X)%*%X,t(X)%*%y) # Testing matrix commands using ordinary least squares
betaHatOLS
plot(x,y)
points <- seq(min(x),max(x),0.1)
points2 <- betaHatOLS[1]+betaHatOLS[2]*points+betaHatOLS[3]*points
lines(points,points2)
X
X = matrix(c(rep(1,n),x,x^2),nrow=n)
X
betaHatOLS = solve(t(X)%*%X,t(X)%*%y) # Testing matrix commands using ordinary least squares
betaHatOLS
points2
points2 <- betaHatOLS[1]+betaHatOLS[2]*points+betaHatOLS[3]*points^2
lines(points,points2)
yHats = rep(0,length(xAxis))
for(i in 1:length(xAxis)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
if(is.invertible(t(X)%*%W%*%X)){  # If it can be inverted, do local linear regression
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*xAxis[i]
} else{                       # If not, do ordinary kernel regression
yHats[i] = sum(K(xAxis[i]-x,h)*y)/sum(K(xAxis[i]-x,h))
}
}
plot(x,y)
lines(x,yHats,type='l')         # Notice less bias on the boundary
for(i in 1:length(xAxis)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
if(is.invertible(t(X)%*%W%*%X)){  # If it can be inverted, do local linear regression
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*xAxis[i]
} else{                       # If not, do ordinary kernel regression
yHats[i] = sum(K(x[i]-x,h)*y)/sum(K(x[i]-x,h))
}
}
plot(x,y)
lines(x,yHats,type='l')         # Notice less bias on the boundary
yHats = rep(0,length(x))
for(i in 1:length(xAxis)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
if(is.invertible(t(X)%*%W%*%X)){  # If it can be inverted, do local linear regression
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*xAxis[i]
} else{                       # If not, do ordinary kernel regression
yHats[i] = sum(K(x[i]-x,h)*y)/sum(K(x[i]-x,h))
}
}
plot(x,y)
lines(x,yHats,type='l')         # Notice less bias on the boundary
lines(x,yTrue,type='l', lty='dotted')
h=1.2
yHats = rep(0,length(x))
for(i in 1:length(x)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
if(is.invertible(t(X)%*%W%*%X)){  # If it can be inverted, do local linear regression
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*xAxis[i]
} else{                       # If not, do ordinary kernel regression
yHats[i] = sum(K(x[i]-x,h)*y)/sum(K(x[i]-x,h))
}
}
plot(x,y)
lines(x,yHats,type='l')         # Notice less bias on the boundary
lines(x,yTrue,type='l', lty='dotted')
X = matrix(c(rep(1,n),x,x^2),nrow=n)
yHats = rep(0,length(x))
X = matrix(c(rep(1,n),x,x^2),nrow=n)
yHats = rep(0,length(x))
h=1.2
for(i in 1:length(x)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
if(is.invertible(t(X)%*%W%*%X)){  # If it can be inverted, do local linear regression
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*x[i]+betaHatWLS[3]*x[i]^2
} else{                       # If not, do ordinary kernel regression
yHats[i] = sum(K(x[i]-x,h)*y)/sum(K(x[i]-x,h))
}
}
yHats
plot(x,y)
lines(x,yHats)
lines(x,yHats,type = "l")
lines(sort(x),yHats,type = "l")
plot(x,y)
lines(sort(x),yHats,type = "l")
plot(x,y)
lines(sort(x),yHats)
plot(x,y)
lines(x,yHats)
yHAts
yHats
for(i in 1:length(x)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*x[i]+betaHatWLS[3]*x[i]^2
}
yHats
X = matrix(c(rep(1,n),sort(x),sort(x)^2),nrow=n)
yHats = rep(0,length(x))
h=1.2
for(i in 1:length(x)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*x[i]+betaHatWLS[3]*x[i]^2
}
plot(x,y)
lines(x,yHats)
X = matrix(c(rep(1,n),x,x^2),nrow=n)
yHats = rep(0,length(x))
h=1.2
for(i in 1:length(x)){          # Now do it for many points on the xAxis
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(X)%*%W%*%X,t(X)%*%W%*%y)
yHats[i] = betaHatWLS[1]+betaHatWLS[2]*x[i]+betaHatWLS[3]*x[i]^2
}
plot(x,y)
lines(x,yHats)
sortedyhats <- yHats[order(sort(x))]
sortedyhats
sortedyhats <- yHats[order(x)]
sortedyhats
lines(sort(x),sortedyhats)
plot(x,y)
sortedyhats <- yHats[order(x)]
lines(sort(x),sortedyhats)
plot(x,y)
lines(sort(x),yHats[order(x)])
G = matrix(c(rep(1,n),nrow=n)
h=1.2
G = matrix(c(rep(1,n),nrow=n))
yHats = rep(0,length(x))
h=1.2
for(i in 1:length(x)){
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(G) %*% W %*% G,t(G) %*% W %*% y)
yHats[i] = betaHatWLS[1]
}
G
G = rep(1,length(x))
yHats = rep(0,length(x))
h=1.2
for(i in 1:length(x)){
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(G) %*% W %*% G,t(G) %*% W %*% y)
yHats[i] = betaHatWLS[1]
}
plot(x,y)
lines(sort(x),yHats[order(x)])
h=5
for(i in 1:length(x)){
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(G) %*% W %*% G,t(G) %*% W %*% y)
yHats[i] = betaHatWLS[1]
}
plot(x,y)
lines(sort(x),yHats[order(x)])
h=1
for(i in 1:length(x)){
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(G) %*% W %*% G,t(G) %*% W %*% y)
yHats[i] = betaHatWLS[1]
}
plot(x,y)
lines(sort(x),yHats[order(x)])
h=0.5
for(i in 1:length(x)){
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(G) %*% W %*% G,t(G) %*% W %*% y)
yHats[i] = betaHatWLS[1]
}
plot(x,y)
lines(sort(x),yHats[order(x)])
K = function(x,h){           # Define the Gaussian kernel
answer = (1/h)*(1/sqrt(2*pi)*exp(-(x/h)^2/2))
}
h=.4             # Try several bandwidth values.  .1,.4,1.2
yHats = rep(0,length(xAxis))
for(i in 1:length(xAxis)){
yHats[i] = sum(K(xAxis[i]-x,h)*y)/sum(K(xAxis[i]-x,h))
}
plot( xAxis,yHats, type='l', ylim=c(-2,35),xlim=range(x))
lines(xAxis,yTrue,type='l', lty='dotted')
points(x,y)
G = rep(1,length(x))
yHats = rep(0,length(x))
h=0.4
for(i in 1:length(x)){
W = diag(K(x[i]-x,h))
betaHatWLS = solve(t(G) %*% W %*% G,t(G) %*% W %*% y)
yHats[i] = betaHatWLS[1]
}
plot(x,y)
lines(sort(x),yHats[order(x)])
t(G) %*% W %*% G
1/(t(G) %*% W %*% G)
(1/(t(G) %*% W %*% G)) %*% t(G)
(1/(t(G) %*% W %*% G)) %*% t(G) %*% y
install.packages('np')
library(np)
bound = 5
A = seq(-bound,bound,.25)
n = length(A)
X = matrix(rep(0,2*n^2),ncol=2)
y = rep(0,n^2)
for (i in 1:n){
for (j in 1:n){
X[(i-1)*n+j,1] = A[i]
X[(i-1)*n+j,2] = A[j]
y[(i-1)*n+j]   = sin(A[i])^2 + sin(A[j])^2 + rnorm(1,mean=0, sd=.5)
}
}
bws = npregbw(xdat = X, ydat = y)  # Figures out bandwidths to use. May take several minutes.
df = data.frame(X,y)
ghat = npreg(bws, xdat = X, ydat = y, xtdat=X, ytdat=y)   # Performs kernel regression
kernplot = plot(ghat, phi = 45)                           # Visualize
install.packages('rpart')
library(rpart)  # For implementing trees. 'R'ecursive 'part'itioning
library(MASS)   # Has a data set used as an example later
x = c(16.9,17.1,12.7,14.4,13.9,16.3,16.0,13.1,12.7,13.4,17.5,14.9,16.0,17.8,15.7,17.2,15.4,12.4)
y = c(12.6,23.7,31.4,11.8,18.0,3.3,  4.6,30.4,30.4,28.4,23.1, 6.6,  .3,35.5,-1.1,18.7,  .2,30.1)
plot(x,y)
options = rpart.control(minbucket=2)
fit = rpart(y~x,control=options, method = "anova")
fit            # Get a basic text readout of the tree
summary(fit)   # Get more information than you wanted
plot(fit, uniform=TRUE, branch = 0)     # Visualize.  Not very useful without text.
text(fit)                               # Label on a node is the condition for going LEFT
plot(x,y)
lines(c(12,13.65),   c(30.14,30.14))
lines(c(17,18.5),    c(25.25,25.25))
lines(c(13.65,14.65),c(14.9,14.9))
lines(c(16.15,17),   c(7.95,7.95))
lines(c(14.65,16.15),c(2.12,2.12))
plotcp(fit)
?birthwt
X = birthwt
head(X)
X$low  = NULL                  # Drop this variable
X$race = as.factor(X$race)     # Convert a few variables to factors
X$smoke = as.factor(X$smoke)
X$ui = as.factor(X$ui)
X$ht = as.factor(X$ht)
fit = rpart(bwt ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data = X)   # Fit the model
fit            # Get a basic text readout of the tree
summary(fit)   # Get more information than you wanted
plot(fit, uniform=TRUE, branch = 0)     # Visualize.  Not very useful without text.
text(fit)                               # Label on a node is the condition for going LEFT
options = rpart.control(cp=.01)   #  Try values .01, .05, and .1.  Use .01 in later prediction.
fit = rpart(bwt ~ age + lwt + smoke + ptl + ht + ui + ftv, data = X, control=options)
plot(fit, uniform=TRUE, branch = 0)
text(fit)
x0 = data.frame(   30,  160, as.factor(0), as.factor(1),     0, as.factor(1), as.factor(0),     1)
names(x0) = c(  'age','lwt',       'race',      'smoke', 'ptl',         'ht',         'ui', 'ftv')
x0
predict(fit, newdata=x0)
plotcp(fit)
prunedFit = prune(fit,cp=.025)
prunedFit
plot(prunedFit, uniform=TRUE, branch=0)
text(prunedFit)
# Try pruning with a higher cp to simplify the tree
par(mar=c(5,6,4,1)+.1)
prunedFit = prune(fit,cp=.025)
prunedFit
plot(prunedFit, uniform=TRUE, branch=0)
text(prunedFit)
# Try pruning with a higher cp to simplify the tree
par(mar=c(5,6,4,1)+.1)
plot(prunedFit, uniform=TRUE, branch=0,height=900)
text(prunedFit)
plot(prunedFit, uniform=TRUE, branch=0,height=400)
plot(prunedFit, uniform=TRUE, branch=0,height=500)
text(prunedFit)
plot(prunedFit, uniform=TRUE, branch=0,height=200)
text(prunedFit)
# Try pruning with a higher cp to simplify the tree
par(mar=c(5,6,4,1)+.1)
plot(prunedFit, uniform=TRUE, branch=0,height=200)
text(prunedFit)
# Try pruning with a higher cp to simplify the tree
par(mar=c(6,6,6,4)+.1)
prunedFit = prune(fit,cp=.025)
prunedFit
plot(prunedFit, uniform=TRUE, branch=0,height=200)
text(prunedFit)
plot(prunedFit, uniform=TRUE, branch=0)
text(prunedFit)
View(prunedFit)
library(MASS)
fit <- rpart(Price ~ Type + MPG.highway + AirBags + Cylinders + Horsepower + Passengers + Length + Weight)
Cars93
fit <- rpart(Price ~ Type + MPG.highway + AirBags + Cylinders + Horsepower + Passengers + Length + Weight, data=Cars93)
plot(fit)
text(fit)
plotcp(fit)
fit <- rpart(Price ~ Type + MPG.highway + AirBags + Cylinders + Horsepower + Passengers + Length + Weight, data=Cars93)
plot(fit)
text(fit)
plotcp(fit)
text(fit)
plot(fit)
text(fit)
library(MASS)
fit <- rpart(Price ~ Type + MPG.highway + AirBags + Cylinders + Horsepower + Passengers + Length + Weight, data=Cars93)
plot(fit)
text(fit)
pdf(file="tree.pdf",width=9,height=9)
plot(fit)
pdf(file="tree.pdf",width=9,height=9)
plot(fit)
pdf(file="tree.pdf",width=9,height=9)
plot(fit)
text(fit)
dev.off()
Cars93
data <- Cars93
View(data)
levels(Cars93)
levels(Cars93$Price)
data1 <- as.matrix(Cars93)
data <- data1[,c(3,5,8:9,11,13,18:19,25)]
data
library(MASS)
fit <- rpart(Price ~ Type + MPG.highway + AirBags + Cylinders + Horsepower + Passengers + Length + Weight, data=Cars93)
plot(fit)
text(fit)
pdf(file="tree.pdf",width=9,height=9)
plot(fit)
text(fit)
dev.off()
pdf(file="tree.pdf",width=9,height=5)
plot(fit)
text(fit)
dev.off()
pdf(file="tree.pdf",width=9,height=6)
plot(fit)
text(fit)
dev.off()
pdf(file="tree.pdf",width=9,height=7)
plot(fit)
text(fit)
dev.off()
setwd("~/Documents/school/Semester 9/Applied Nonparametric Statistics 7132 - A/HW 28")
setwd("~/Documents/school/Semester 9/Applied Nonparametric Statistics 7132 - A/HW 28")
library(MASS)
fit <- rpart(Price ~ Type + MPG.highway + AirBags + Cylinders + Horsepower + Passengers + Length + Weight, data=Cars93)
pdf(file="tree.pdf",width=9,height=7)
plot(fit)
text(fit)
dev.off()
levels(Cars93$AirBags)
levels.default(Cars93$AirBags)
`levels<-`(Cars93$AirBags,c)
levels(Cars93$Price)
plotcp(fit)
png("cpplot.png",width = 5,height = 3)
plotcp(fit)
dev.off()
png("cpplot.png")
plotcp(fit)
dev.off()
pdf("cpplot.png")
plotcp(fit)
dev.off()
pdf("cpplot.pdf")
plotcp(fit)
dev.off()
prunedFit = prune(fit,cp=.11)
prunedFit
plot(prunedFit, uniform=TRUE, branch=0)
text(prunedFit)
plot(prunedFit, uniform=TRUE)
text(prunedFit)
plot(prunedFit)
text(prunedFit)
text(prunedFit)
text(prunedFit)
text(prunedFit)
plot(prunedFit, uniform=TRUE, branch=0)
text(prunedFit)
plot(prunedFit, uniform=TRUE)
pdf("prune.pdf",width = 9,height = 5)
plot(prunedFit, uniform=TRUE)
text(prunedFit)
dev.off()
pdf("prune.pdf",width = 9,height = 7)
plot(prunedFit, uniform=TRUE)
text(prunedFit)
dev.off()
setwd("C:/Users/rl02898/Documents/Bayesian Statistics/Final Project")
setwd("~/Documents/school/Semester 9/Intro to Bayesian Inference 7090 - A/Project")
data1 <- read.csv("Drug_Overdose_Deaths.csv",header = TRUE,sep = ",")
a <- data1[data1$Year=="2018" & data1$Indicator=="Number of Drug Overdose Deaths",]
b <- as.matrix(a)
c <- b[,-c(2:5,7:12)]
d <- transform(c, Data.Value = as.numeric(Data.Value))
e <- aggregate(Data.Value~State,d,sum)
f <- e[-c(45,53),]           ## Finally done sorting the large data set
Populations <- c(737438,4887871,3013825,7171646,39865590,5695564,
3572665,702455,967171,21299325,10519475,1420491,
3156145,1754208,12741080,6691878,2911505,4468402,
4659978,6902149,6042718,1338404,9995915,5611179,
6126452,2986530,1062305,10383620,760077,1929268,
1356458,8908520,2095428,3034392,19542209,11689442,
3943079,4190713,12807060,1057315,5084127,882235,
6770010,29865997,3161105,8517685,626299,7535591,
5813568,1805832,577737)
Region <- c(1,5,5,3,2,3,7,6,6,5,5,2,4,1,4,4,4,4,5,7,6,7,4,4,4,
5,1,5,4,4,7,6,3,2,6,4,3,1,6,7,5,4,5,3,3,6,7,1,4,6,1)
Income <- c(73181,48123,45869,56581,71805,69117,74168,82372,
62852,52594,56183,77765,58570,52225,62992,54181,
56422,48375,46145,77385,80776,55277,54909,68388,
53578,43529,53386,52752,61843,59970,73381,80088,
46744,58003,64894,54021,50051,60212,59105,63870,
50570,56521,51340,59206,65358,71535,57513,70979,
59305,43469,60434)
Health.Care <- c(11064,7281,7408,6452,7549,6804,9859,11944,10254,
8076,6587,7299,8200,6927,8262,8300,7651,8004,7815,
10559,8602,9531,8055,8871,8107,7646,8221,7264,9851,
8412,9589,8859,7214,6714,9778,8712,7627,8044,9258,
9551,7311,8933,7372,6998,5982,7556,10190,7913,8702,
9462,8320
)
Percent.Pop <- (f[,2]/Populations)*100
data2 <- cbind(f,Percent.Pop,Region,Income,Health.Care)
row.names(data2) <- 1:nrow(data2)
colnames(data2) <- c("State","Deaths","Percent.Pop","Region","Income","Health.Care")
data <- data2[,3]
View(data1)
data1
data2
View(data1)
d
f
data2
## Let's first look at a frequentist approach to get a feel for the data.
linreg <- lm(data2[,3]~data2[,4]+data2[,5]+data2[,6])
summary(linreg)
linreg1 <- lm(data2[,3]~data2[,2])
summary(linreg1)
data[,2]
data2[,2]
data2
